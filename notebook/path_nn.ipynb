{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pprint\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/bwxtsw2d1kg3s67q9jnsnly52sglqb/T/ipykernel_30855/2668232990.py:3: DtypeWarning: Columns (24,25,26,27,28,29,30,31,32,33,34,35,36,37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(ptn_paths, skiprows=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: AQ定義, データ数:37\n",
      "class: IF定義, データ数:4\n",
      "class: DBテーブル, データ数:11\n",
      "class: メッセージ一覧, データ数:5\n",
      "class: CSVレイアウト(画面), データ数:5\n",
      "class: 電文レイアウト, データ数:6\n",
      "class: 画面, データ数:231\n",
      "class: 画面遷移図, データ数:24\n",
      "class: 帳票レイアウト, データ数:8\n",
      "class: CRUD図, データ数:6\n",
      "class: プログラム設計, データ数:1911\n",
      "class: 画面レイアウト, データ数:80\n",
      "class: 千手定義, データ数:52\n",
      "Total: 2535\n"
     ]
    }
   ],
   "source": [
    "def ptn_label_load():\n",
    "    ptn_paths = \"PTN共有_保振ホール決済/NPL-68_設計書種別ラベリング_試験用ラベル追加.csv\"\n",
    "    return pd.read_csv(ptn_paths, skiprows=1)\n",
    "\n",
    "ptn_df = ptn_label_load()\n",
    "use_category = \"設計書種別(試験用)\" # \"設計書種別\" or \"設計書種別(再整理)\" or \"設計書種別(試験用)\"\n",
    "ptn_df = ptn_df[(ptn_df[use_category].notna())]\n",
    "len(ptn_df)\n",
    "\n",
    "def convert_df(x):\n",
    "    label = x[use_category]\n",
    "    path = \"/\".join(x[\"ALL\"].split(\"/\")[6:])+x[\"title\"]\n",
    "    return (label, path)\n",
    "\n",
    "data_list = ptn_df.apply(convert_df, axis=1).tolist()\n",
    "\n",
    "data_dict = {}\n",
    "for data in data_list:\n",
    "    if data[0] in data_dict.keys():\n",
    "        data_dict[data[0]].append(data[1])\n",
    "    else:\n",
    "        data_dict[data[0]] = [data[1]]\n",
    "del data_dict[\"nolabel\"]\n",
    "# del data_dict[\"画面(JavaScript)\"]\n",
    "[print(f\"class: {key}, データ数:{len(data_dict[key])}\") for key in data_dict.keys()]\n",
    "print(f\"Total: {len(data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mojimoji\n",
    "import MeCab\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"izumi-lab/electra-base-japanese-discriminator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    result = mojimoji.han_to_zen(text)\n",
    "    result = mojimoji.zen_to_han(result, kana=False)\n",
    "    result.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n",
    "    return result\n",
    "\n",
    "def text_to_words(text):\n",
    "    #text = normalize_text(text)\n",
    "    m_text = tokenizer.tokenize(text)\n",
    "    return m_text\n",
    "\n",
    "\n",
    "def text_to_phrase(text):\n",
    "    return \" \".join(text.split(\"/\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'AQ定義', 2: 'IF定義', 3: 'DBテーブル', 4: 'メッセージ一覧', 5: 'CSVレイアウト(画面)', 6: '電文レイアウト', 7: '画面', 8: '画面遷移図', 9: '帳票レイアウト', 10: 'CRUD図', 11: 'プログラム設計', 12: '画面レイアウト', 13: '千手定義'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words = []\n",
    "corpus_phrase = []\n",
    "corpus_origin = []\n",
    "target_label = []\n",
    "\n",
    "label_dic = {}\n",
    "label2idx = {}\n",
    "for ind, label in enumerate(data_dict.keys()):\n",
    "    label_dic[ind+1] = label\n",
    "    label2idx[label] = ind+1\n",
    "\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    for item in data_dict[key]:\n",
    "        text = normalize_text(item)\n",
    "        # corpus_words.append(text_to_words(text))\n",
    "        # corpus_phrase.append(text_to_phrase(text))\n",
    "        corpus_origin.append(text)\n",
    "        target_label.append(label2idx[key])\n",
    "\n",
    "\n",
    "\n",
    "print(label_dic)\n",
    "target_label[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 未知後の検出\n",
    "unk_flag = False\n",
    "unk_words = []\n",
    "for ind in tqdm(range(len(corpus_words))):\n",
    "    sub_words = corpus_words[ind]\n",
    "    origin_words = corpus_phrase[ind]\n",
    "    for subw in sub_words:\n",
    "        if subw in origin_words:\n",
    "            if unk_flag:\n",
    "                unk_word = origin_words.split(subw, 1)[0]\n",
    "                origin_words = origin_words.split(subw, 1)[-1]\n",
    "                unk_words.append(unk_word)\n",
    "            else:\n",
    "                origin_words = origin_words.split(subw, 1)[-1]\n",
    "            \n",
    "        elif \"#\" in subw:\n",
    "            subw_ = re.sub(\"#\", \"\", subw)\n",
    "            origin_words = origin_words.split(subw_, 1)[-1]\n",
    "        elif \"[UNK]\" == subw:\n",
    "            unk_flag = True\n",
    "\n",
    "unk_words = list(set(unk_words))\n",
    "\n",
    "print(len(unk_words))\n",
    "for x in unk_words:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at izumi-lab/electra-base-japanese-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at izumi-lab/electra-base-japanese-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"izumi-lab/electra-base-japanese-discriminator\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"izumi-lab/electra-base-japanese-discriminator\", num_labels=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUD図:310_[ST]ホール決済/030 外部設計/020 インターフェース関連/01_AQ・IFレイアウト/01_AQ関連/01_STAR内部/02_受信/AQ_003_国内株式約定済連絡データ(WADKGV00/WADSGV00/WADKGV20).xls\n",
      "CRUD図:310_[ST]ホール決済/030 外部設計/020.インターフェース関連/01_AQ・IFレイアウト/01_AQ関連/01_STAR内部/02_受信/(ST)(WAF)001_WAFYIK00_外国株式約定データ.xlsx\n",
      "CRUD図:310_[ST]ホール決済/030 外部設計/020.インターフェース関連/01_AQ・IFレイアウト/01_AQ関連/01_STAR内部/02_受信/(ST)(WAF)003_WAFYIT00_外国投信約定データ.xlsx\n",
      "CRUD図:310_[ST]ホール決済/030 外部設計/020.インターフェース関連/01_AQ・IFレイアウト/01_AQ関連/01_STAR内部/02_受信/(ST)(WAF)004_WAFGIK00_外国株式約定済連絡データ.xls\n",
      "CRUD図:310_[ST]ホール決済/030 外部設計/020.インターフェース関連/01_AQ・IFレイアウト/01_AQ関連/01_STAR内部/02_受信/(ST)(WAF)007_WAFDIK00_外国株式出来連絡データ.xls\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for words in corpus_origin[:5]:\n",
    "        text = \"\".join(words)\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\",padding=True, truncation=True)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        print(f\"{label_dic[logits.argmax(1).item()]}:{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 7, 11]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus_origin, target_label, test_size=0.3, shuffle=True, random_state=0, stratify=target_label)\n",
    "y_train[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = x_train[:20], x_test[:20], y_train[:20], y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 7, 11, 11, 11, 11, 11, 11]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/var/pyenv/versions/3.9.5/envs/search-engine/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:09<01:26,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss:2.680373430252075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:18<01:12,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:26<01:02,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:37<00:57,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:46<00:47,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:54<00:35,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:02<00:25,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:09<00:16,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:17<00:08,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:25<00:00,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, loss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "epochs = 10\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "#再学習\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    inputs = tokenizer.batch_encode_plus(x_train, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs, labels=torch.tensor(y_train))\n",
    "    logits = outputs.logits\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss)\n",
    "    print(f\"epoch:{epoch+1}, loss:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x154e4aa60>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習過程の表示\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:画面遷移図\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:画面レイアウト\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:プログラム設計\n",
      "AQ定義:画面遷移図\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for index, words in enumerate(x_test):\n",
    "        text = \"\".join(words)\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\",padding=True, truncation=True)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        pred=logits.argmax(1).item()\n",
    "        preds.append(pred)\n",
    "        #print(f\"{label_dic[logits.argmax(1).item()+1]}:{label_dic[y_test[index]]}, input: {text}, \")\n",
    "        print(f\"{label_dic[logits.argmax(1).item()+1]}:{label_dic[y_test[index]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5955dc64dc45c9f6ad89241066402e9b90de6090510d28e2354caffb11d22191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
